# Adversarial-attacks-on-CNN-MNIST-Classifier
FGSM and One-Pixel adversarial attacks on built and trained CNN MNIST Image classifier.

Colab Notebook - https://colab.research.google.com/drive/10RMASamClV00tLBk0Y7aPWW0kx34yQt5?usp=sharing
Jovian - https://jovian.ai/deepankdixit0804/fooling-an-mnist-classifier

What are Adversarial attacks: 
Machine learning models are vulnerable to adversarial examples, which are data points chosen very close to examples in the training set such that they are misclassified by the trained model. As part of this project, I completed: 
a. Train a machine learning model and study its vulnerability to various types of adversarial attacks. 
b. Explore ways to decrease the modelâ€™s vulnerability to adversarial examples and demonstrate their effectiveness.


Reference: Adversarial Attacks and Defenses in Images, Graphs and Text: A Review, Xu et al. 2019. (https://arxiv.org/abs/1909.08072)
